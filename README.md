# Data Mapper API

A Python FastAPI application that accepts documents (CSV, Excel, JSON, XML) with mapping configurations and maps the data to a PostgreSQL database according to the specified schema.

## Features

- ğŸ“ **Multi-Format Support**: CSV, Excel, JSON, and XML file processing
- ğŸ—„ï¸ **Dynamic Schema Creation**: Automatically creates database tables based on configuration
- ğŸ”„ **Flexible Data Mapping**: Configurable field mappings with transformation rules
- ğŸš€ **High Performance**: Chunked processing with parallel duplicate checking for large files
- â˜ï¸ **Cloud Integration**: Direct integration with Backblaze B2 storage
- ğŸ” **Smart Duplicate Detection**: File-level and row-level duplicate checking
- ğŸ’¬ **Natural Language Queries**: AI-powered console for database queries
- âš¡ **Async Processing**: Background task processing for long-running operations
- ğŸ“Š **RESTful API**: Complete REST API with automatic documentation

## Quick Start

### Prerequisites

- Python 3.8+
- PostgreSQL 12+
- Docker & Docker Compose (optional)

### Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd content-atlas
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Start PostgreSQL**
   ```bash
   docker-compose up -d db
   ```

4. **Configure environment**
   ```bash
   # Create .env file
   echo "DATABASE_URL=postgresql://postgres:postgres@localhost:5432/datamapper" > .env
   ```

5. **Run the application**
   ```bash
   uvicorn app.main:app --reload
   ```

6. **Access the API**
   - API: http://localhost:8000
   - Interactive Docs: http://localhost:8000/docs

## Documentation

### Getting Started
- ğŸ“– [Setup Guide](docs/SETUP.md) - Detailed installation and configuration
- ğŸ—ï¸ [Architecture Overview](docs/ARCHITECTURE.md) - System design and components
- ğŸ§ª [Testing Guide](docs/TESTING.md) - Testing strategies and examples

### API Documentation
- ğŸ”Œ [API Reference](docs/API_REFERENCE.md) - Complete endpoint documentation
- ğŸ”„ [Duplicate Detection](docs/DUPLICATE_DETECTION.md) - Duplicate detection system
- âš¡ [Parallel Processing](docs/PARALLEL_PROCESSING.md) - Large file processing

### Operations
- ğŸš€ [Deployment Guide](docs/DEPLOYMENT.md) - Production deployment
- ğŸ’» [Console Interface](docs/CONSOLE.md) - Natural language query console
- ğŸ”„ [Database Reset](docs/DATABASE_RESET.md) - Reset database for testing

## API Endpoints

### Data Import
- `POST /map-data` - Upload and map file data
- `POST /map-b2-data` - Import from Backblaze B2
- `POST /map-b2-data-async` - Async processing for large files

### Data Analysis
- `POST /detect-b2-mapping` - Auto-detect schema from file
- `POST /extract-b2-excel-csv` - Preview Excel file contents

### Table Management
- `GET /tables` - List all tables
- `GET /tables/{table_name}` - Query table data
- `GET /tables/{table_name}/schema` - Get table schema
- `GET /tables/{table_name}/stats` - Get table statistics

### Task Management
- `GET /tasks/{task_id}` - Check async task status

See [API Reference](docs/API_REFERENCE.md) for detailed documentation.

### Archive Auto-Process (ZIP)
- `POST /auto-process-archive` downloads the ZIP from B2 once, streams each supported entry (CSV/XLSX/XLS) from memory for analysis/import, and uploads each entry a single time for persistence.
- Reprocessing a specific file later uses the stored B2 path for that entry (not the whole ZIP), so day-to-day runs avoid per-entry B2 downloads and stay under bandwidth caps.
- Unsupported entries (non-CSV/Excel) are marked as skipped; processing results and any failures are recorded on the import job.
- Entries are fingerprinted by structure (normalized headers/column count) so the first analyzed file seeds the LLM decision and matching siblings reuse that mapping and target table automaticallyâ€”keeping grouped files together and avoiding duplicate analysis work while preserving archive order.

## Usage Example

```bash
# Upload a CSV file with mapping configuration
curl -X POST "http://localhost:8000/map-data" \
  -F "file=@customers.csv" \
  -F 'mapping_json={
    "table_name": "customers",
    "db_schema": {
      "id": "INTEGER",
      "name": "VARCHAR(255)",
      "email": "VARCHAR(255)"
    },
    "mappings": {
      "id": "Customer ID",
      "name": "Customer Name",
      "email": "Email Address"
    }
  }'
```

## Console Interface

Query your database using natural language:

```bash
# Start interactive console
python -m app.console

# Or run a single query
python -m app.console "Show me all customers"
```

See [Console Interface Guide](docs/CONSOLE.md) for more details.

## Docker Deployment

### Quick Start with Docker Compose

```bash
# Start the complete stack
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

See [Deployment Guide](docs/DEPLOYMENT.md) for production deployment.

## Environment Variables

### Required
- `DATABASE_URL` - PostgreSQL connection string

### Initial admin setup
- The first account created through the `/register` page becomes the admin. Automatic bootstrap via `ADMIN_*` variables has been removed; create the initial admin interactively when the app first loads.

### Optional (for B2 integration)
- `B2_APPLICATION_KEY_ID` - Backblaze B2 key ID
- `B2_APPLICATION_KEY` - Backblaze B2 application key
- `B2_BUCKET_NAME` - B2 bucket name

### Optional (for console)
- `ANTHROPIC_API_KEY` - Anthropic API key for natural language queries

## Development

### Running Tests

```bash
# Run tests
pytest

# Run with coverage
pytest --cov=app

# Run specific tests
pytest tests/test_api.py -v

# Skip B2 analysis tests (e.g., when B2/LLM creds unavailable)
pytest -m "not b2"

# Start development server
uvicorn app.main:app --reload
```

### Database Reset (Development Only)

Reset the database to a clean state while preserving user accounts:

```bash
# Interactive mode with confirmation
python reset_dev_db.py

# Auto-confirm (for automation)
python reset_dev_db.py --yes
```

This will:
- Drop all user-created data tables
- Clear tracking tables (file_imports, table_metadata, import_history, import_jobs, uploaded_files)
- Delete all files from B2 storage
- **Preserve** user accounts

âš ï¸ **Production Safety**: The script automatically detects and blocks production environments.

See [Database Reset Guide](docs/DATABASE_RESET.md) for detailed documentation.

## Repository Structure

Content Atlas uses a **monorepo for development** but automatically syncs to separate deployment repositories for independent scaling.

### Development (This Repository)
- **Unified development environment** with full codebase
- **Automated sync** to deployment repositories via GitHub Actions
- **Local development** with docker-compose

### Deployment Repositories
- **`content-atlas-api`**: FastAPI backend for serverless/container deployment
- **`content-atlas-frontend`**: React frontend for static hosting (Vercel, Netlify)

ğŸ“– **[Repository Separation Guide](docs/REPOSITORY_SEPARATION.md)** - Learn how the automated sync works

## Project Structure

```
content-atlas/                    # Development monorepo
â”œâ”€â”€ app/                          # FastAPI backend
â”‚   â”œâ”€â”€ main.py                   # Application entry point
â”‚   â”œâ”€â”€ api/                      # REST API endpoints
â”‚   â”œâ”€â”€ core/                     # Core functionality
â”‚   â”œâ”€â”€ db/                       # Database models & session
â”‚   â”œâ”€â”€ domain/                   # Business logic
â”‚   â””â”€â”€ utils/                    # Utilities
â”œâ”€â”€ frontend/                     # React frontend
â”‚   â”œâ”€â”€ src/                      # React application
â”‚   â”œâ”€â”€ public/                   # Static assets
â”‚   â””â”€â”€ package.json              # Dependencies
â”œâ”€â”€ docs/                         # Documentation
â”œâ”€â”€ tests/                        # Test suite
â”œâ”€â”€ docker-compose.yml            # Local development
â”œâ”€â”€ .github/workflows/            # Automated sync workflows
â””â”€â”€ setup-separate-repos.sh       # Repository setup script
```

## Key Features Explained

### Duplicate Detection
Intelligent duplicate detection at both file and row levels. Configure uniqueness columns, enable/disable checks, and customize error messages. See [Duplicate Detection](docs/DUPLICATE_DETECTION.md).

### Large File Processing
Automatic chunked processing for files >10,000 records with parallel duplicate checking. Handles files up to 100MB+ efficiently. See [Parallel Processing](docs/PARALLEL_PROCESSING.md).

### Dynamic Schema Creation
Tables are created automatically based on your mapping configuration. Supports multiple data types including INTEGER, VARCHAR, DECIMAL, and TIMESTAMP.

### Cloud Storage Integration
Direct integration with Backblaze B2 for importing files from cloud storage without manual downloads.

## Performance

- **Small files** (<1,000 records): <2 seconds
- **Medium files** (1,000-10,000 records): 2-10 seconds
- **Large files** (10,000-50,000 records): 10-30 seconds
- **Very large files** (>50,000 records): 30+ seconds with async processing

## Contributing

Contributions are welcome! Please read our contributing guidelines and submit pull requests.

## License

[Your License Here]

## Support

For issues, questions, or contributions:
- ğŸ“– Check the [documentation](docs/)
- ğŸ› Report bugs via GitHub issues
- ğŸ’¬ Ask questions in discussions

## Related Projects

- [FastAPI](https://fastapi.tiangolo.com/) - Web framework
- [Pandas](https://pandas.pydata.org/) - Data processing
- [PostgreSQL](https://www.postgresql.org/) - Database
- [Backblaze B2](https://www.backblaze.com/b2/) - Cloud storage
